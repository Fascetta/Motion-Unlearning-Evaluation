{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Antonio\\anaconda3\\envs\\salad\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from t2m import Text2Motion\n",
    "from utils.get_opt import get_opt\n",
    "from utils.fixseed import fixseed\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from os.path import join as pjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading checkpoints/t2m/t2m_denoiser_vpred_vaegelu/opt.txt\n",
      "Reading checkpoints/t2m/t2m_vae_gelu/opt.txt\n",
      "Loading VAE Model t2m_vae_gelu\n",
      "Loading Denoiser Model t2m_denoiser_vpred_vaegelu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP text encoder version ViT-B/32\n",
      "Reading ./checkpoints/t2m/Comp_v6_KLD005/opt.txt\n"
     ]
    }
   ],
   "source": [
    "denoiser_name = \"t2m_denoiser_vpred_vaegelu\"\n",
    "dataset_name = \"t2m\"\n",
    "generator = Text2Motion(denoiser_name, dataset_name)\n",
    "\n",
    "opt = generator.opt\n",
    "wrapper_opt = get_opt(opt.dataset_opt_path, torch.device(\"cuda\"))\n",
    "mean = np.load(pjoin(wrapper_opt.meta_dir, \"mean.npy\"))\n",
    "std = np.load(pjoin(wrapper_opt.meta_dir, \"std.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denoiser_name = \"kit_denoiser_vpred_vaegelu_bsz16\" # Point to your specific KIT checkpoint\n",
    "# dataset_name = \"kit\"                                   # Tell the system to use KIT-ML logic\n",
    "\n",
    "# generator = Text2Motion(denoiser_name, dataset_name)\n",
    "\n",
    "# opt = generator.opt\n",
    "# wrapper_opt = get_opt(opt.dataset_opt_path, torch.device(\"cuda\"))\n",
    "# mean = np.load(pjoin(wrapper_opt.meta_dir, \"mean.npy\"))\n",
    "# std = np.load(pjoin(wrapper_opt.meta_dir, \"std.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixseed(42)\n",
    "src_text = \"a man is dancing\"\n",
    "m_lens = 64\n",
    "cfg_scale = 7.5\n",
    "num_inference_timesteps = 50\n",
    "\n",
    "init_noise, src_motion, (sa, ta, ca) = generator.generate(src_text,\n",
    "                                                          m_lens,\n",
    "                                                          cfg_scale,\n",
    "                                                          num_inference_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit - 4 Different Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit_text = \"slowly\"\n",
    "edit_text = \"a really desperate man walking around\"\n",
    "src_proportion = 0.2\n",
    "\n",
    "# # case 1: mirror\n",
    "# edit_motion = generator.edit(init_noise,\n",
    "#                              src_text=src_text,\n",
    "#                              edit_text=edit_text,\n",
    "#                              edit_mode=\"mirror\",\n",
    "#                              mirror_mode=\"lower\",\n",
    "#                              cfg_scale=cfg_scale,\n",
    "#                              num_inference_timesteps=num_inference_timesteps,\n",
    "#                              src_sa=sa,\n",
    "#                              src_ta=ta,\n",
    "#                              src_ca=ca,\n",
    "#                              src_proportion=src_proportion)\n",
    "\n",
    "# # case 2: reweight\n",
    "# edit_motion = generator.edit(init_noise,\n",
    "#                              src_text=src_text,\n",
    "#                              edit_text=src_text,\n",
    "#                              edit_mode=\"reweight\",\n",
    "#                              tgt_word=\"high\",\n",
    "#                              reweight_scale=-1.0,\n",
    "#                              cfg_scale=cfg_scale,\n",
    "#                              num_inference_timesteps=num_inference_timesteps,\n",
    "#                              src_sa=sa,\n",
    "#                              src_ta=ta,\n",
    "#                              src_ca=ca,\n",
    "#                              src_proportion=src_proportion)\n",
    "\n",
    "# case 3: refine\n",
    "edit_motion = generator.edit(init_noise,\n",
    "                             src_text=src_text,\n",
    "                             edit_text=edit_text,\n",
    "                             edit_mode=\"refine\",\n",
    "                             cfg_scale=cfg_scale,\n",
    "                             num_inference_timesteps=num_inference_timesteps,\n",
    "                             src_sa=sa,\n",
    "                             src_ta=ta,\n",
    "                             src_ca=ca,\n",
    "                             src_proportion=src_proportion)\n",
    "\n",
    "# # case 4: word swap\n",
    "# edit_motion = generator.edit(init_noise,\n",
    "#                              src_text=src_text,\n",
    "#                              edit_text=edit_text,\n",
    "#                              edit_mode=\"word_swap\",\n",
    "#                              cfg_scale=cfg_scale,\n",
    "#                              num_inference_timesteps=num_inference_timesteps,\n",
    "#                              src_sa=sa,\n",
    "#                              src_ta=None,\n",
    "#                              src_ca=ca,\n",
    "#                              src_proportion=src_proportion,\n",
    "#                              swap_src_proportion=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./checkpoints/t2m/Comp_v6_KLD005/opt.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join as pjoin\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.motion_process import recover_from_ric\n",
    "from utils.plot_script import plot_3d_motion\n",
    "from utils.get_opt import get_opt\n",
    "\n",
    "def plot_t2m(data, text, filename):\n",
    "    os.makedirs(\"edit_result\", exist_ok=True)\n",
    "    #data = data[:m_lens[0].item()]\n",
    "    data = data[:m_lens]\n",
    "    joint = recover_from_ric(torch.from_numpy(data).float(), opt.joints_num).numpy()\n",
    "    save_path = pjoin(\"edit_result\", f\"{filename}.mp4\")\n",
    "    plot_3d_motion(save_path, opt.kinematic_chain, joint, title=text, fps=20)\n",
    "\n",
    "    np.save(pjoin(\"edit_result\", f\"{filename}_pos.npy\"), joint)\n",
    "    np.save(pjoin(\"edit_result\", f\"{filename}_feats.npy\"), data)\n",
    "    \n",
    "# mean and std for de-normalization\n",
    "wrapper_opt = get_opt(opt.dataset_opt_path, torch.device('cuda'))\n",
    "mean = np.load(pjoin(wrapper_opt.meta_dir, 'mean.npy'))\n",
    "std = np.load(pjoin(wrapper_opt.meta_dir, 'std.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_motion = src_motion.detach().cpu().numpy() * std + mean\n",
    "plot_t2m(src_motion[0], src_text, \"src\")\n",
    "\n",
    "edit_motion = edit_motion.detach().cpu().numpy()* std + mean\n",
    "plot_t2m(edit_motion[0], edit_text, \"edit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running FFmpeg...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"edit_result\\final.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from IPython.display import Video\n",
    "\n",
    "# Define paths\n",
    "src_path = os.path.join(\"edit_result\", \"src.mp4\")\n",
    "edit_path = os.path.join(\"edit_result\", \"edit.mp4\")\n",
    "out_path = os.path.join(\"edit_result\", \"final.mp4\")\n",
    "\n",
    "# Build the ffmpeg command\n",
    "# -y: overwrite output\n",
    "# -filter_complex hstack: stack video streams horizontally\n",
    "cmd = [\n",
    "    \"ffmpeg\",\n",
    "    \"-y\",\n",
    "    \"-i\", src_path,\n",
    "    \"-i\", edit_path,\n",
    "    \"-filter_complex\", \"hstack\",\n",
    "    out_path\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "print(\"Running FFmpeg...\")\n",
    "subprocess.run(cmd, check=True)\n",
    "\n",
    "# Display\n",
    "Video(out_path, width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNLEARNING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing CLIP model layers...\n",
      "Starting ESD Unlearning on 13 concepts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0060: 100%|██████████| 150/150 [00:13<00:00, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlearning complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Denoiser(\n",
       "  (input_process): InputProcess(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (output_process): OutputProcess(\n",
       "    (layers): Sequential(\n",
       "      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=256, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (timestep_emb): TimestepEmbedding(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (clip_model): FrozenCLIPTextEncoder(\n",
       "    (model): CLIPModel(\n",
       "      (text_model): CLIPTextTransformer(\n",
       "        (embeddings): CLIPTextEmbeddings(\n",
       "          (token_embedding): Embedding(49408, 512)\n",
       "          (position_embedding): Embedding(77, 512)\n",
       "        )\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (4): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (5): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (6): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (7): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (8): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (9): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (10): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (11): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (vision_model): CLIPVisionTransformer(\n",
       "        (embeddings): CLIPVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "          (position_embedding): Embedding(50, 768)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (4): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (5): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (6): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (7): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (8): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (9): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (10): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (11): CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "      (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (word_emb): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (pos_emb): PositionalEmbedding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): SkipTransformer(\n",
       "    (input_blocks): ModuleList(\n",
       "      (0): STTransformerLayer(\n",
       "        (skel_attn): MultiheadAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (skel_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (skel_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (temp_attn): MultiheadAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (temp_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (temp_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (cross_attn): MultiheadAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (cross_src_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_tgt_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ffn_linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (ffn_linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (skel_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (temp_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (cross_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ffn_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): STTransformerLayer(\n",
       "        (skel_attn): MultiheadAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (skel_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (skel_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (temp_attn): MultiheadAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (temp_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (temp_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (cross_attn): MultiheadAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (cross_src_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_tgt_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ffn_linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (ffn_linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (skel_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (temp_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (cross_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ffn_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (middle_block): STTransformerLayer(\n",
       "      (skel_attn): MultiheadAttention(\n",
       "        (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (skel_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (skel_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (temp_attn): MultiheadAttention(\n",
       "        (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (temp_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (temp_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (cross_attn): MultiheadAttention(\n",
       "        (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (cross_src_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (cross_tgt_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (cross_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ffn_linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (ffn_linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (skel_film): DenseFiLM(\n",
       "        (linear): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (temp_film): DenseFiLM(\n",
       "        (linear): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (cross_film): DenseFiLM(\n",
       "        (linear): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ffn_film): DenseFiLM(\n",
       "        (linear): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_blocks): ModuleList(\n",
       "      (0): STTransformerLayer(\n",
       "        (skel_attn): MultiheadAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (skel_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (skel_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (temp_attn): MultiheadAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (temp_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (temp_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (cross_attn): MultiheadAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (cross_src_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_tgt_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ffn_linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (ffn_linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (skel_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (temp_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (cross_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ffn_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): STTransformerLayer(\n",
       "        (skel_attn): MultiheadAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (skel_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (skel_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (temp_attn): MultiheadAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (temp_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (temp_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (cross_attn): MultiheadAttention(\n",
       "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (cross_src_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_tgt_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ffn_linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (ffn_linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (skel_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (temp_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (cross_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ffn_film): DenseFiLM(\n",
       "          (linear): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (skip_blocks): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. Configuration\n",
    "# ==========================================\n",
    "forget_prompts = [\n",
    "    \"a man kicking\", \"kicking someone\", \"violent kick\",\n",
    "    \"punching\", \"a man punching\", \"fighting\",\n",
    "    \"attacking\", \"violence\", \"hitting\", \"combat\",\n",
    "    \"martial arts\", \"strike\", \"beating\"\n",
    "]\n",
    "neutral_prompt = \"standing\" # Empty string = generic/unconditional motion\n",
    "\n",
    "# Training params\n",
    "learning_rate = 1e-5\n",
    "num_steps = 150        # ESD usually works quickly (100-300 steps)\n",
    "batch_size = 4         # Keep small for VRAM\n",
    "motion_length = 64     # Length in frames\n",
    "\n",
    "# ==========================================\n",
    "# 2. Setup Optimizer & Freeze CLIP\n",
    "# ==========================================\n",
    "# The denoiser is where the weights we want to change are located\n",
    "denoiser = generator.denoiser\n",
    "device = generator.device\n",
    "\n",
    "# Freeze the CLIP text encoder to prevent destroying language understanding\n",
    "# In t2m.py, CLIP is stored at generator.denoiser.clip_model\n",
    "if hasattr(denoiser, 'clip_model'):\n",
    "    print(\"Freezing CLIP model layers...\")\n",
    "    denoiser.clip_model.eval()\n",
    "    for param in denoiser.clip_model.parameters():\n",
    "        param.requires_grad = False\n",
    "else:\n",
    "    print(\"Warning: clip_model not found on denoiser. Check architecture.\")\n",
    "\n",
    "# Ensure denoiser is in training mode\n",
    "denoiser.train()\n",
    "\n",
    "# We only optimize the denoiser parameters (excluding the frozen CLIP)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, denoiser.parameters()), lr=learning_rate)\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "print(f\"Starting ESD Unlearning on {len(forget_prompts)} concepts...\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Training Loop\n",
    "# ==========================================\n",
    "pbar = tqdm(range(num_steps))\n",
    "\n",
    "for step in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # --- A. Prepare Data ---\n",
    "    # Sample prompts\n",
    "    current_prompts = np.random.choice(forget_prompts, batch_size).tolist()\n",
    "    neutral_prompts = [neutral_prompt] * batch_size\n",
    "    \n",
    "    # --- B. Create Latent Noise ---\n",
    "    # Based on t2m.py generate(): z shape is (Batch, m_lens//4, 7, latent_dim)\n",
    "    # The VAE compresses motion by factor of 4 roughly? Or architecture specific.\n",
    "    # We use m_lens // 4 as seen in the 'generate' method.\n",
    "    dim_latent = generator.vae_opt.latent_dim\n",
    "    # Shape: [Batch, Frames, Joints, Dim]\n",
    "    # Note: Text2Motion.generate uses (1, m_lens//4, 7, dim). We match that pattern.\n",
    "    z = torch.randn(batch_size, motion_length // 4, 7, dim_latent).to(device)\n",
    "    \n",
    "    # --- C. Sample Timesteps ---\n",
    "    # We need random timesteps for diffusion training\n",
    "    # generator.opt.num_train_timesteps usually holds the max steps (e.g. 1000)\n",
    "    max_timesteps = generator.opt.num_train_timesteps\n",
    "    t = torch.randint(0, max_timesteps, (batch_size,), device=device).long()\n",
    "    \n",
    "    # --- D. Forward Pass (ESD Logic) ---\n",
    "    # t2m.py denoiser.forward takes: (z, timestep, text_list, need_attn)\n",
    "    # It returns (prediction, attention_tuple). We only need prediction [0].\n",
    "    \n",
    "    # 1. Prediction for \"Violence\" (The behavior we want to change)\n",
    "    pred_forget, _ = denoiser(z, t, current_prompts, need_attn=False)\n",
    "    \n",
    "    # 2. Prediction for \"Neutral\" (The target behavior)\n",
    "    # We detach this because we don't want to learn the neutral concept, \n",
    "    # we just want the violent concept to POINT to this output.\n",
    "    with torch.no_grad():\n",
    "        pred_neutral, _ = denoiser(z, t, neutral_prompts, need_attn=False)\n",
    "\n",
    "    # --- E. Loss & Backprop ---\n",
    "    # Minimize difference between \"Kick\" output and \"Neutral\" output\n",
    "    loss = mse_loss(pred_forget, pred_neutral)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Unlearning complete.\")\n",
    "denoiser.eval() # Switch back to eval for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"edit_result\\after_unlearning.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Force a specific seed to compare cleanly\n",
    "from utils.fixseed import fixseed\n",
    "fixseed(42)\n",
    "\n",
    "# Use a prompt from the forget set\n",
    "test_text = \"a man kicking\" \n",
    "\n",
    "# Generate\n",
    "# We assume 'generator' is still the main object instance\n",
    "# and 'denoiser' (which we just trained) is linked to it.\n",
    "init_noise, src_motion, (sa, ta, ca) = generator.generate(test_text, \n",
    "                                                          m_lens=64, \n",
    "                                                          cfg_scale=7.5, \n",
    "                                                          num_inference_timesteps=50)\n",
    "\n",
    "# De-normalize and plot\n",
    "src_motion = src_motion.detach().cpu().numpy() * std + mean\n",
    "plot_t2m(src_motion[0], test_text, \"after_unlearning\")\n",
    "\n",
    "# Show video\n",
    "from IPython.display import Video\n",
    "import os\n",
    "Video(os.path.join(\"edit_result\", \"after_unlearning.mp4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading original model for 'Before' generation...\n",
      "Reading checkpoints/t2m/t2m_denoiser_vpred_vaegelu/opt.txt\n",
      "Reading checkpoints/t2m/t2m_vae_gelu/opt.txt\n",
      "Loading VAE Model t2m_vae_gelu\n",
      "Loading Denoiser Model t2m_denoiser_vpred_vaegelu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP text encoder version ViT-B/32\n",
      "\n",
      "Starting Unlearning Process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unlearning: 100%|██████████| 300/300 [00:23<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 'After' motion...\n",
      "\n",
      "Stitching videos...\n",
      "Displaying comparison:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"edit_result\\comparison_unlearning.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "from IPython.display import Video\n",
    "from utils.fixseed import fixseed\n",
    "from os.path import join as pjoin\n",
    "\n",
    "# --- Configuration ---\n",
    "test_prompt = \"a man kicking violently\"\n",
    "comparison_filename = \"comparison_unlearning.mp4\"\n",
    "\n",
    "# ==========================================\n",
    "# Step 1: Generate \"Before\" (Original Model)\n",
    "# ==========================================\n",
    "# We must reload the model to ensure it is clean\n",
    "print(\"Reloading original model for 'Before' generation...\")\n",
    "generator = Text2Motion(denoiser_name, dataset_name) # Reloads weights from disk\n",
    "\n",
    "fixseed(42)\n",
    "_, before_motion, _ = generator.generate(test_prompt, m_lens=64, cfg_scale=7.5)\n",
    "\n",
    "# Save \"Before\" visualization\n",
    "before_motion = before_motion.detach().cpu().numpy() * std + mean\n",
    "plot_t2m(before_motion[0], f\"Before: {test_prompt}\", \"before_unlearning\")\n",
    "\n",
    "# ==========================================\n",
    "# Step 2: Perform ESD Unlearning\n",
    "# ==========================================\n",
    "print(\"\\nStarting Unlearning Process...\")\n",
    "\n",
    "# Unlearning Configuration\n",
    "forget_prompts = [\"kick\", \"punch\", \"figh\", \"strike\", \"violence\"]\n",
    "neutral_prompt = \"standing\" \n",
    "learning_rate = 1e-5 # Slightly higher for quicker results in demo\n",
    "num_steps = 300\n",
    "batch_size = 4\n",
    "\n",
    "# Setup Optimization\n",
    "denoiser = generator.denoiser\n",
    "denoiser.train()\n",
    "# Freeze CLIP\n",
    "if hasattr(denoiser, 'clip_model'):\n",
    "    denoiser.clip_model.eval()\n",
    "    for param in denoiser.clip_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, denoiser.parameters()), lr=learning_rate)\n",
    "mse_loss = nn.MSELoss()\n",
    "device = generator.device\n",
    "\n",
    "# Training Loop\n",
    "for step in tqdm(range(num_steps), desc=\"Unlearning\"):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1. Inputs\n",
    "    current_prompts = np.random.choice(forget_prompts, batch_size).tolist()\n",
    "    neutral_prompts = [neutral_prompt] * batch_size\n",
    "    \n",
    "    # 2. Noise (Match shape: Batch, Frames//4, Joints, Dim)\n",
    "    z = torch.randn(batch_size, 64 // 4, 7, generator.vae_opt.latent_dim).to(device)\n",
    "    t = torch.randint(0, generator.opt.num_train_timesteps, (batch_size,), device=device).long()\n",
    "    \n",
    "    # 3. ESD Loss\n",
    "    pred_forget, _ = denoiser(z, t, current_prompts, need_attn=False)\n",
    "    with torch.no_grad():\n",
    "        pred_neutral, _ = denoiser(z, t, neutral_prompts, need_attn=False)\n",
    "\n",
    "    loss = mse_loss(pred_forget, pred_neutral)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "denoiser.eval() # Switch back to eval\n",
    "\n",
    "# ==========================================\n",
    "# Step 3: Generate \"After\" (Unlearned Model)\n",
    "# ==========================================\n",
    "print(\"\\nGenerating 'After' motion...\")\n",
    "fixseed(42) # Use SAME seed to see exact impact\n",
    "_, after_motion, _ = generator.generate(test_prompt, m_lens=64, cfg_scale=7.5)\n",
    "\n",
    "# Save \"After\" visualization\n",
    "after_motion = after_motion.detach().cpu().numpy() * std + mean\n",
    "plot_t2m(after_motion[0], f\"After: {test_prompt}\", \"after_unlearning\")\n",
    "\n",
    "# ==========================================\n",
    "# Step 4: Create Side-by-Side Video\n",
    "# ==========================================\n",
    "print(\"\\nStitching videos...\")\n",
    "path_before = pjoin(\"edit_result\", \"before_unlearning.mp4\")\n",
    "path_after = pjoin(\"edit_result\", \"after_unlearning.mp4\")\n",
    "path_out = pjoin(\"edit_result\", comparison_filename)\n",
    "\n",
    "cmd = [\n",
    "    \"ffmpeg\", \"-y\",\n",
    "    \"-i\", path_before,\n",
    "    \"-i\", path_after,\n",
    "    \"-filter_complex\", \"[0:v][1:v]hstack=inputs=2[v]\", # Stack horizontally\n",
    "    \"-map\", \"[v]\",\n",
    "    path_out\n",
    "]\n",
    "subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "print(\"Displaying comparison:\")\n",
    "Video(path_out, width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
